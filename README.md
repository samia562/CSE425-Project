This project applies Variational Autoencoders (VAE) for hybrid music clustering by combining both audio and lyrics features. The aim is to learn a joint latent space that allows for effective clustering of music tracks by genre and language. The audio features are extracted using MFCC, while the lyrics are processed using TF-IDF and SVD. The model is evaluated using clustering metrics such as Silhouette Score, Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI). The results demonstrate that incorporating both audio and text data improves clustering accuracy compared to using audio features alone.

The project includes synthetic music data with 4000 vocal-only 30-second clips in English and Bangla. The clustering is performed using methods like K-Means, DBSCAN, and Agglomerative Clustering, and the latent space is visualized using t-SNE. The repository contains the implementation of the VAE, data preprocessing, clustering, and evaluation scripts, along with results such as visualizations and clustering metrics.
